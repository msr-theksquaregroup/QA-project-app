\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{enumitem}

\title{Detailed Information Flow in the Agentic Test Automation Framework}
\author{Alberto Espinosa}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
This document details the data flow in an agentic test automation framework orchestrated by LangGraph for analyzing, transforming, and executing automated tests for web applications. The system processes existing test code (e.g., Cypress or Playwright scripts) and generates enhanced artifacts like Gherkin features, test plans, Playwright code, execution results, and coverage reports.

\begin{itemize}
    \item \textbf{Structure:} Information flows sequentially through 8 agents via a shared \texttt{TestAutomationState} (a TypedDict).
    \item \textbf{Process:} Each agent consumes prior outputs, generates new data, and updates the state.
    \item \textbf{Flow:} Starts from input processing, progresses through agent-based transformations, and ends with artifact generation.
    \item \textbf{User Interaction:} Outputs are visible via console logs and files saved in \texttt{output\_13/} folders.
\end{itemize}

\section{Overall Flow Structure}
\begin{itemize}
    \item \textbf{Pipeline:} Linear flow: Input $\rightarrow$ Analysis $\rightarrow$ Agents 1-8 $\rightarrow$ Outputs.
    \item \textbf{Data Generation:}
        \begin{itemize}
            \item Parsing: Uses regex and AST for code analysis.
            \item LLM: Employs Groq/LangChain for natural language generation (e.g., user stories).
            \item Execution: Runs tests via Node.js subprocesses with Playwright/c8.
        \end{itemize}
    \item \textbf{Purpose:} Transforms raw test code into structured, executable, and reportable artifacts.
    \item \textbf{User Visibility:} Console logs show progress; artifacts (e.g., JSON reports, PNG visuals) are saved to folders.
\end{itemize}

\section{Graph}

\subsection{Pre-Pipeline: Input Folder Processor}
\begin{itemize}
    \item \textbf{What It Does:} Scans a specified folder for test code files and reads their content to initiate the pipeline.
    \item \textbf{How It Does It:} Recursively traverses the input folder (e.g., \texttt{input\_data\_1}) using Python's \texttt{os} module, identifies supported file extensions (.js, .ts, .py, etc.), and reads file contents with proper encoding handling (UTF-8 with retries for encoding errors).
    \item \textbf{Inputs:} Folder path provided by the user.
    \item \textbf{Outputs:} List of dictionaries per file: \texttt{\{filename, code\_content\}}.
    \item \textbf{Role in System:} Acts as the entry point, collecting raw test code for further analysis. Ensures all relevant files are captured to feed into the EnhancedCodeAnalyzer, setting the stage for the agentic workflow.
\end{itemize}

\subsection{Pre-Pipeline: EnhancedCodeAnalyzer}
\begin{itemize}
    \item \textbf{What It Does:} Analyzes test code to extract key elements like URLs, test steps, languages, and frameworks.
    \item \textbf{How It Does It:} Uses regex patterns to parse code:
        \begin{itemize}
            \item Extracts real URLs (e.g., from \texttt{cy.visit('https://example.com')}).
            \item Parses test steps (e.g., \texttt{cy.get(sel).click()} $\rightarrow$ dict \{'type': 'click', 'selector': sel\}).
            \item Detects languages via file extensions (e.g., .js $\rightarrow$ JavaScript; supports Python, Java, etc.).
            \item Identifies frameworks (e.g., Cypress, Playwright, Selenium, Jest) by matching keywords (\texttt{cy.}, \texttt{page.goto}) and imports.
        \end{itemize}
        Handles chained commands (e.g., splitting input + assertion) and fixes assertion parsing (e.g., \texttt{cy.url().should('eq', url)} $\rightarrow$ 'assert\_url').
    \item \textbf{Inputs:} List of \texttt{\{filename, code\_content\}} from Input Folder Processor.
    \item \textbf{Outputs:} \texttt{ast\_analysis} dict per file: \texttt{\{real\_urls: list, parsed\_steps: list of dicts, language, frameworks\}}.
    \item \textbf{Role in System:} Provides structured data as the foundation for all agents. Ensures accurate extraction of real URLs and steps, enabling subsequent generation of artifacts like Gherkin features and Playwright code.
\end{itemize}

\subsection{Agent 1: Code Analysis}
\begin{itemize}
    \item \textbf{What It Does:} Refines the initial code analysis to improve accuracy of extracted data.
    \item \textbf{How It Does It:} Processes \texttt{ast\_analysis} to:
        \begin{itemize}
            \item Enhance URL extraction (deduplicates, validates format).
            \item Refine step parsing (e.g., corrects chained assertions like \texttt{cy.get().should('have.value')}).
            \item Add metrics (e.g., step count, assertion types).
        \end{itemize}
        Uses rule-based logic to ensure consistency (e.g., standardizing selector formats).
    \item \textbf{Inputs:} \texttt{TestAutomationState} with \texttt{original\_code, filename, ast\_analysis}.
    \item \textbf{Outputs:} Updated \texttt{ast\_analysis} in state (enhanced dict with URLs, steps, metrics).
    \item \textbf{Role in System:} Acts as a quality gate, ensuring robust data for downstream agents. Its refined analysis directly informs narrative generation (e.g., user stories) and test transformations.
\end{itemize}

\subsection{Agent 2: User Story}
\begin{itemize}
    \item \textbf{What It Does:} Generates a human-readable user story describing the test's purpose.
    \item \textbf{How It Does It:} 
        \begin{itemize}
            \item If a user story file is provided, reads it directly.
            \item Otherwise, uses LangChain/Groq (Llama3 model) to generate a story via a prompt like "Generate a user story based on test steps: \{steps\}."
            \item Fallback (if API unavailable): Uses a hardcoded template (e.g., "As a [role], I want to... based on \{analysis\_summary\}").
        \end{itemize}
    \item \textbf{Inputs:} \texttt{TestAutomationState} with \texttt{ast\_analysis} (and optional \texttt{user\_story\_file}).
    \item \textbf{Outputs:} \texttt{user\_story} string (e.g., "As a user, I want to log in to verify credentials").
    \item \textbf{Role in System:} Bridges technical test steps to business-readable narratives, feeding into Gherkin generation for BDD alignment. Ensures tests are contextualized for stakeholders.
\end{itemize}

\subsection{Agent 3: Gherkin}
\begin{itemize}
    \item \textbf{What It Does:} Creates a Gherkin feature file for behavior-driven development (BDD).
    \item \textbf{How It Does It:} 
        \begin{itemize}
            \item Maps parsed steps to Gherkin syntax (e.g., \texttt{'type': 'click'} $\rightarrow$ "When I click on \{selector\}").
            \item Uses real URLs from \texttt{ast\_analysis} in the Background (e.g., "Given I am on \{primary\_url\}").
            \item Employs LLM for natural language phrasing or falls back to templates if API fails.
        \end{itemize}
    \item \textbf{Inputs:} \texttt{TestAutomationState} with \texttt{ast\_analysis, user\_story}.
    \item \textbf{Outputs:} \texttt{gherkin\_feature} string (e.g., Feature file content with Feature, Background, Scenario).
    \item \textbf{Role in System:} Translates technical steps into structured BDD specs, enabling collaboration between developers and non-technical stakeholders. Feeds structured scenarios to the test plan agent.
\end{itemize}

\subsection{Agent 4: Test Plan}
\begin{itemize}
    \item \textbf{What It Does:} Produces a detailed test plan in Markdown format.
    \item \textbf{How It Does It:} 
        \begin{itemize}
            \item Uses LLM to expand \texttt{user\_story} and \texttt{gherkin\_feature} into a structured document (sections: prerequisites, test cases, expected outcomes).
            \item Fallback: Template-based plan using parsed steps (e.g., "Test Case: Verify \{action\} on \{selector\}").
        \end{itemize}
    \item \textbf{Inputs:} \texttt{TestAutomationState} with \texttt{user\_story, gherkin\_feature}.
    \item \textbf{Outputs:} \texttt{test\_plan} string (Markdown content).
    \item \textbf{Role in System:} Provides a human-readable plan for test execution, guiding the generation of Playwright code and ensuring traceability from requirements to tests.
\end{itemize}

\subsection{Agent 5: Playwright}
\begin{itemize}
    \item \textbf{What It Does:} Generates executable Playwright test code in JavaScript.
    \item \textbf{How It Does It:} 
        \begin{itemize}
            \item Maps parsed steps to Playwright syntax (e.g., \texttt{'type': 'click'} $\rightarrow$ \texttt{page.locator(sel).click()}).
            \item Ensures consistent locators (e.g., \texttt{page.locator()}) and fixed assertion handling (e.g., \texttt{expect(page.locator()).toHaveText()}).
            \item Uses \texttt{ast\_analysis} for steps and \texttt{test\_plan} for context.
        \end{itemize}
    \item \textbf{Inputs:} \texttt{TestAutomationState} with \texttt{ast\_analysis, test\_plan}.
    \item \textbf{Outputs:} \texttt{playwright\_code} string (.spec.js file content).
    \item \textbf{Role in System:} Enables migration from other frameworks (e.g., Cypress) to Playwright, producing modern, executable tests for real-time validation.
\end{itemize}

\subsection{Agent 6: Execution}
\begin{itemize}
    \item \textbf{What It Does:} Runs the generated Playwright tests and collects results/coverage.
    \item \textbf{How It Does It:} 
        \begin{itemize}
            \item Sets up a Node.js project with Playwright and c8 (V8 coverage tool).
            \item Executes tests via Python \texttt{subprocess} calls to Node.js, running \texttt{npx playwright test}.
            \item Collects raw coverage JSON from c8 and execution logs (pass/fail, errors).
            \item If Node.js fails (e.g., not installed), marks results as "not collected."
        \end{itemize}
    \item \textbf{Inputs:} \texttt{TestAutomationState} with \texttt{playwright\_code}.
    \item \textbf{Outputs:} \texttt{execution\_result} dict (status, time, \texttt{coverage\_collected}: bool, logs).
    \item \textbf{Role in System:} Validates generated tests in real browsers, providing real-world execution data and coverage metrics for quality assessment.
\end{itemize}

\subsection{Agent 7: Coverage}
\begin{itemize}
    \item \textbf{What It Does:} Processes execution results to generate coverage reports and visualizations.
    \item \textbf{How It Does It:} 
        \begin{itemize}
            \item Parses c8 coverage JSON to compute percentages (lines, functions, branches).
            \item Generates HTML reports and PNG charts using Matplotlib/Seaborn.
            \item If no coverage data (e.g., Node.js failure), returns \texttt{\{"coverage\_collected": False, "overall\_percentage": 0.0\}}.
        \end{itemize}
    \item \textbf{Inputs:} \texttt{TestAutomationState} with \texttt{execution\_result}.
    \item \textbf{Outputs:} \texttt{coverage\_report} dict (percentages, file paths); \texttt{coverage\_image\_path} (PNG file).
    \item \textbf{Role in System:} Quantifies test effectiveness via coverage metrics, highlighting untested code areas for quality assurance.
\end{itemize}

\subsection{Agent 8: Final Report}
\begin{itemize}
    \item \textbf{What It Does:} Compiles a comprehensive summary of all artifacts and saves them to output folders.
    \item \textbf{How It Does It:} 
        \begin{itemize}
            \item Aggregates all state data (user story, Gherkin, test plan, code, results, coverage).
            \item Generates a JSON summary with metadata (e.g., timestamp, errors).
            \item Saves artifacts to \texttt{output\_13/} subfolders (e.g., \texttt{features/}, \texttt{tests/}, \texttt{coverage/}).
        \end{itemize}
    \item \textbf{Inputs:} Full \texttt{TestAutomationState} (all prior outputs).
    \item \textbf{Outputs:} \texttt{final\_report} dict (summary); \texttt{artifacts} dict (file paths).
    \item \textbf{Role in System:} Concludes the pipeline, delivering all outputs in a structured format for user review and integration into development workflows.
\end{itemize}

\section{Error Handling and Shared State}
\begin{itemize}
    \item \textbf{Error Handling:} Exceptions (e.g., LLM or Node.js failures) are caught and appended to the \texttt{errors} list in the state. Fallbacks (e.g., templates for LLM, "not collected" for coverage) ensure pipeline continuity.
    \item \textbf{Shared State:} \texttt{TestAutomationState} persists data across agents, ensuring traceability and enabling each agent to build on prior outputs.
\end{itemize}

\section{User Interaction and Visibility}
\begin{itemize}
    \item \textbf{Execution:} Users run notebook cells sequentially in a Jupyter environment.
    \item \textbf{Outputs:} Console logs display progress (e.g., "Agent X: Generated Y characters"); artifacts are saved to folders (e.g., viewable as .feature, .js, .md, .html, or .png files).
    \item \textbf{Future Enhancement:} A web UI could provide dashboards for easier interaction and visualization.
\end{itemize}

\section{Next Stages, Enhancements, Lacks, and Planning}
\subsection{Current Strengths}
\begin{itemize}
    \item Robust agentic design: 8 agents in a clean LangGraph pipeline with shared state.
    \item Fixes implemented: Real URLs, assertion parsing, locator consistency, real coverage.
    \item Flexibility: Supports multiple languages/frameworks; includes fallbacks.
    \item Outputs: Generates 18+ files per run (e.g., Gherkin, tests, reports, visuals).
\end{itemize}

\subsection{Weaknesses}
\begin{itemize}
    \item \textbf{Scalability:} Notebook-based; lacks parallelism for file processing.
    \item \textbf{Input/Output Variety:} File-based inputs; no API/web UI or database.
    \item \textbf{Advanced AI:} Prompts are basic; no multi-agent collaboration.
    \item \textbf{Testing Depth:} Coverage is JavaScript-focused (V8/c8); limited for non-JS languages.
    \item \textbf{Security/Compliance:} No handling of sensitive data (e.g., API keys in URLs).
    \item \textbf{Monitoring:} Lacks metrics tracking (e.g., LLM costs, execution times).
\end{itemize}

\subsection{Next Stages (Short-Term, 1-3 Months)}
\begin{enumerate}
    \item \textbf{Refactor to App:} Convert to Streamlit/Flask web app for user-friendly interface.
    \item \textbf{Parallel Processing:} Implement multiprocessing for files; async for LLM calls.
    \item \textbf{Enhance LLM:} Fine-tune prompts; integrate advanced models (e.g., GPT-4) or RAG.
    \item \textbf{Expand Support:} Add generators for Selenium, Pytest; support non-web tests (API/mobile).
    \item \textbf{Testing Suite:} Write unit tests for classes/agents; simulate Node.js failures.
\end{enumerate}

\subsection{Enhancements (Medium-Term, 3-6 Months)}
\begin{itemize}
    \item \textbf{Multi-Language Execution:} Add runners for Python (Pytest), Java (JUnit) with coverage tools.
    \item \textbf{AI Improvements:} Enable collaborative agents (e.g., feedback loops); integrate code interpreters.
    \item \textbf{Visualization/Dashboard:} Build a web dashboard for aggregated reports.
    \item \textbf{Integration:} Add CI/CD hooks (e.g., GitHub Actions); API endpoints.
    \item \textbf{Error Resilience:} Implement auto-retry for executions; ML-based anomaly detection.
\end{itemize}

\subsection{Long-Term Planning (6+ Months)}
\begin{itemize}
    \item \textbf{Commercialization:} Open-source (GitHub) or SaaS (e.g., Vercel) with premium LLM pricing.
    \item \textbf{Community/Extensions:} Plugin system for custom agents; user feedback for prompts.
    \item \textbf{Metrics Tracking:} Add analytics (e.g., success rates); benchmark vs. manual testing.
    \item \textbf{Roadmap:} v1.0: Web app. v2.0: Multi-language execution. v3.0: AI optimization (e.g., auto-fix coverage gaps).
    \item \textbf{Team Needs:} 2-3 Python/AI developers, 1 tester, 1 UI designer.
\end{itemize}

\end{document}