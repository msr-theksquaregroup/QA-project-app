{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4dd302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import TypedDict, Dict, Any, Optional\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, END\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import re\n",
    "import subprocess\n",
    "import shutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "from functools import wraps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0073618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Environment Setup & Configuration\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"GROQ_API_KEY not found in .env file. Please set it.\")\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_FOLDER = \"input_data_output_16\"\n",
    "LOGS_FOLDER_NAME = \"logs\"\n",
    "REPORTS_FOLDER_NAME = \"reports\"\n",
    "\n",
    "# Create main output directory\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "log_filename = os.path.join(OUTPUT_FOLDER, LOGS_FOLDER_NAME,\n",
    "                            f\"universal_converter_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "os.makedirs(os.path.join(OUTPUT_FOLDER, LOGS_FOLDER_NAME), exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename, encoding=\"utf-8\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbe6600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. State Definition\n",
    "class AgentState(TypedDict):\n",
    "    user_story: str\n",
    "    code_input: str\n",
    "    gherkin_feature: str\n",
    "    analyzed_code: str\n",
    "    test_plan: str\n",
    "    playwright_code: str\n",
    "    review_feedback: str\n",
    "    final_playwright_code: str\n",
    "    code_path: str\n",
    "    test_results: Optional[str]\n",
    "    test_passed: Optional[bool]\n",
    "    coverage_report: Optional[str]\n",
    "    coverage_percentage: Optional[float]\n",
    "    output_dir: str\n",
    "    artifacts: Dict[str, str]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff080cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize LLM\n",
    "def get_llm(temperature=0.1):\n",
    "    \"\"\"Initializes and returns the ChatGroq LLM.\"\"\"\n",
    "    return ChatGroq(\n",
    "        groq_api_key=GROQ_API_KEY,\n",
    "        model_name=\"llama3-70b-8192\",\n",
    "        temperature=temperature,\n",
    "        max_tokens=4096,\n",
    "        request_timeout=60\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d9ad756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Enhanced Retry Decorator\n",
    "def with_retry(func=None, *, max_attempts=3, base_delay=3):\n",
    "    def decorator(inner_func):\n",
    "        @wraps(inner_func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(1, max_attempts + 1):\n",
    "                try:\n",
    "                    return inner_func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    delay = base_delay * (2 ** (attempt - 1))\n",
    "                    if \"429\" in str(e) or \"Too Many Requests\" in str(e):\n",
    "                        logger.warning(f\"Rate limit hit in {inner_func.__name__}, waiting {delay}s before retry {attempt}/{max_attempts}\")\n",
    "                    else:\n",
    "                        logger.warning(f\"Attempt {attempt} failed in {inner_func.__name__}: {e}\")\n",
    "                    if attempt == max_attempts:\n",
    "                        logger.error(f\"Max retries reached for {inner_func.__name__}. Error: {e}\")\n",
    "                        raise\n",
    "                    time.sleep(delay)\n",
    "        return wrapper\n",
    "    return decorator(func) if func else decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5cbf7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Helper Functions\n",
    "def save_artifact(artifact_dir: str, filename: str, content: str) -> str:\n",
    "    \"\"\"Save content to a file in the specified directory.\"\"\"\n",
    "    if not content or not content.strip():\n",
    "        logger.warning(f\"Skipping empty artifact: {filename}\")\n",
    "        return \"\"\n",
    "    \n",
    "    os.makedirs(artifact_dir, exist_ok=True)\n",
    "    file_path = os.path.join(artifact_dir, filename)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    logger.info(f\"Saved artifact: {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def save_json_artifact(artifact_dir: str, filename: str, data: dict) -> str:\n",
    "    \"\"\"Save JSON data to a file in the specified directory.\"\"\"\n",
    "    if not data:\n",
    "        logger.warning(f\"Skipping empty JSON artifact: {filename}\")\n",
    "        return \"\"\n",
    "    \n",
    "    os.makedirs(artifact_dir, exist_ok=True)\n",
    "    file_path = os.path.join(artifact_dir, filename)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    logger.info(f\"Saved JSON artifact: {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def load_test_cases(input_folder: str = \"input_data_1\") -> list:\n",
    "    \"\"\"Load test cases from the input folder.\"\"\"\n",
    "    test_cases = []\n",
    "    if not os.path.exists(input_folder):\n",
    "        logger.error(f\"Input folder '{input_folder}' not found.\")\n",
    "        return []\n",
    "    logger.info(f\"Loading test cases from: {input_folder}\")\n",
    "    for item in os.listdir(input_folder):\n",
    "        item_path = os.path.join(input_folder, item)\n",
    "        if not os.path.isdir(item_path):\n",
    "            continue\n",
    "        try:\n",
    "            user_story_path = os.path.join(item_path, \"user_story.txt\")\n",
    "            user_story = \"\"\n",
    "            if os.path.exists(user_story_path):\n",
    "                with open(user_story_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    user_story = f.read().strip()\n",
    "            code_input, code_path = \"\", \"\"\n",
    "            code_extensions = (\".js\", \".jsx\", \".ts\", \".tsx\", \".cy.js\", \".spec.js\", \".test.js\")\n",
    "            for file_name in os.listdir(item_path):\n",
    "                if file_name.endswith(code_extensions):\n",
    "                    code_path = os.path.join(item_path, file_name)\n",
    "                    with open(code_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        code_input = f.read().strip()\n",
    "                    break\n",
    "            if code_input:\n",
    "                test_cases.append({\n",
    "                    \"name\": item,\n",
    "                    \"user_story\": user_story,\n",
    "                    \"code_input\": code_input,\n",
    "                    \"code_path\": code_path\n",
    "                })\n",
    "                logger.info(f\"Loaded: {item}\")\n",
    "            else:\n",
    "                logger.warning(f\"Incomplete: {item}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading {item}: {e}\")\n",
    "    logger.info(f\" Total loaded: {len(test_cases)} test cases\")\n",
    "    return test_cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d5c57bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Agent Definitions\n",
    "@with_retry\n",
    "def synthetic_user_story_generator(state: AgentState) -> dict:\n",
    "    \"\"\"Generate synthetic user stories from source code if user_story is missing.\"\"\"\n",
    "    logger.info(\" Agent: SyntheticUserStoryGenerator - Creating user story from code.\")\n",
    "    \n",
    "    if state.get(\"user_story\", \"\").strip():\n",
    "        logger.info(\"User story already exists. Skipping synthetic generation.\")\n",
    "        return {}\n",
    "    if not state.get(\"code_input\", \"\").strip():\n",
    "        logger.warning(\" Cannot generate user story: no code input available.\")\n",
    "        synthetic_story = \"As a user, I want the app to work, so that I can complete tasks.\"\n",
    "    else:\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an AI trained to write user stories from raw frontend code. \n",
    "            Generate a realistic user story describing what the code does, as if it were written by a product owner.\n",
    "            Format: \"As a [role], I want to [goal], so that [benefit].\"\n",
    "            Return ONLY the user story as plain text.\"\"\"),\n",
    "            (\"user\", \"Here is the source code:\\n{code_input}\")\n",
    "        ])\n",
    "        chain = prompt | get_llm(0.7) | StrOutputParser()\n",
    "        synthetic_story = chain.invoke({\"code_input\": state[\"code_input\"]}).strip()\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return {\n",
    "        \"user_story\": synthetic_story,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99265e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_retry\n",
    "def gherkin_converter(state: AgentState) -> dict:\n",
    "    \"\"\"Convert a user story into a Gherkin .feature file format.\"\"\"\n",
    "    logger.info(\"Agent: GherkinConverter - Converting user story to Gherkin BDD format.\")\n",
    "    if not state.get(\"user_story\", \"\").strip():\n",
    "        return {\"gherkin_feature\": \"# Feature: No user story provided\\n# Scenario: Empty input\"}\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Convert the given user story into a well-structured Gherkin .feature file.\n",
    "        Requirements:\n",
    "        - Create descriptive Feature and Scenario names\n",
    "        - Use proper Given/When/Then structure\n",
    "        - Use 2-space indentation\n",
    "        - Return only the raw Gherkin content without markdown formatting\"\"\"),\n",
    "        (\"user\", \"Convert this user story to Gherkin:\\n{user_story}\")\n",
    "    ])\n",
    "    chain = prompt | get_llm(0.7) | StrOutputParser()\n",
    "    feature = chain.invoke({\"user_story\": state[\"user_story\"]})\n",
    "    feature = feature.strip()\n",
    "    \n",
    "    return {\n",
    "        \"gherkin_feature\": feature,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f165bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_retry\n",
    "def code_analyzer(state: AgentState) -> dict:\n",
    "    \"\"\"Analyze frontend code to extract key information for testing.\"\"\"\n",
    "    logger.info(\"Agent: CodeAnalyzer - Analyzing input code.\")\n",
    "    if not state.get(\"code_input\", \"\").strip():\n",
    "        return {\"analyzed_code\": \"No code input was provided for analysis.\"}\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Analyze the provided code and extract key testing elements.\n",
    "        Focus on:\n",
    "        1. Purpose and main functionality\n",
    "        2. User interactions (clicks, form submissions, navigation)\n",
    "        3. Selectors (CSS, IDs, class names used)\n",
    "        4. Assertions or test conditions\n",
    "        5. API calls and navigation URLs\n",
    "        6. Form fields and input elements\n",
    "        7. Test data values used in the original code\n",
    "        Provide a concise, structured analysis in plain text. Preserve all original test data values exactly as they appear in the code.\"\"\"),\n",
    "        (\"user\", \"Analyze this code for testing:\\n{code_input}\")\n",
    "    ])\n",
    "    chain = prompt | get_llm(0.3) | StrOutputParser()\n",
    "    analysis = chain.invoke({\"code_input\": state[\"code_input\"]})\n",
    "    analysis = analysis.strip()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    return {\n",
    "        \"analyzed_code\": analysis,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "062ce2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_retry\n",
    "def test_plan_generator(state: AgentState) -> dict:\n",
    "    \"\"\"Create a detailed test plan for Playwright.\"\"\"\n",
    "    logger.info(\"Agent: TestPlanGenerator - Creating comprehensive test plan.\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Create a detailed Playwright test plan based on the Gherkin feature and code analysis.\n",
    "        Write it as a simple text description without code blocks or markdown.\n",
    "        CRITICAL REQUIREMENTS:\n",
    "        1. Follow the original code structure exactly - do not add describe blocks, beforeEach blocks, or any other structure not present in the original code\n",
    "        2. Use the exact same test data values as in the original code\n",
    "        3. Include only the actions and assertions that are present in the original code\n",
    "        4. Do not add any additional steps, assertions, or actions\n",
    "        5. Map Cypress commands directly to their Playwright equivalents:\n",
    "           - cy.visit() â†’ page.goto()\n",
    "           - cy.url().should() â†’ expect(page).toHaveURL()\n",
    "           - cy.get().type() â†’ page.locator().fill()\n",
    "           - cy.get().should('have.value') â†’ expect(page.locator()).toHaveValue()\n",
    "           - cy.get().click() â†’ page.locator().click()\n",
    "        6. Preserve the order of operations exactly as in the original code\n",
    "        7. DO NOT add any waitFor, waitForLoadState, or other wait methods that weren't in the original code\n",
    "        Include:\n",
    "        1. Navigation (page URLs and routing)\n",
    "        2. Locators (specific selectors to use, exactly as in the original)\n",
    "        3. Actions (step-by-step user interactions, exactly as in the original)\n",
    "        4. Assertions (what to verify at each step, exactly as in the original)\n",
    "        Write as actionable instructions, not code.\"\"\"),\n",
    "        (\"user\", \"Create test plan based on:\\nGherkin Feature:\\n{gherkin_feature}\\nCode Analysis:\\n{analyzed_code}\")\n",
    "    ])\n",
    "    chain = prompt | get_llm(0.5) | StrOutputParser()\n",
    "    plan = chain.invoke({\n",
    "        \"gherkin_feature\": state[\"gherkin_feature\"],\n",
    "        \"analyzed_code\": state[\"analyzed_code\"]\n",
    "    }).strip()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    return {\n",
    "        \"test_plan\": plan,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50b756be",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_retry\n",
    "def playwright_code_generator(state: AgentState) -> dict:\n",
    "    \"\"\"Generate executable Playwright code from the test plan.\"\"\"\n",
    "    logger.info(\"Agent: PlaywrightCodeGenerator - Generating executable test code.\")\n",
    "    test_plan = state.get(\"test_plan\", \"\").strip()\n",
    "    if not test_plan:\n",
    "        logger.error(\"No test plan provided\")\n",
    "        return {\"playwright_code\": \"// Error: No test plan provided\"}\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a Playwright expert. Generate a complete, executable test file (.spec.js) based on the test plan.\n",
    "        CRITICAL REQUIREMENTS:\n",
    "        - Start with: const {{ test, expect }} = require('@playwright/test');\n",
    "        - Use a simple test() structure - NO test.describe() blocks unless explicitly mentioned in the plan\n",
    "        - Use proper async/await syntax\n",
    "        - Include the exact test name from the original code\n",
    "        - Use robust Playwright locators (page.locator, page.getByRole, etc.)\n",
    "        - Add proper assertions with expect()\n",
    "        - Use the EXACT same test data values as in the original code\n",
    "        - Include ONLY the actions and assertions that were in the original code\n",
    "        - DO NOT add any additional steps, assertions, or actions\n",
    "        - DO NOT add any comments that weren't in the original code\n",
    "        - DO NOT add any explanatory text before or after the code\n",
    "        - DO NOT add any waitFor, waitForLoadState, or other wait methods that weren't in the original code\n",
    "        - Return ONLY the raw JavaScript code without any formatting or explanations\n",
    "        - The output should be exactly the code that can be saved as a .spec.js file and executed\n",
    "        Example structure:\n",
    "        const {{ test, expect }} = require('@playwright/test');\n",
    "        test('test description from original', async ({{ page }}) => {{\n",
    "          await page.goto('url');\n",
    "          await page.locator('selector').fill('exact value from original');\n",
    "          await expect(page.locator('selector')).toHaveValue('exact value from original');\n",
    "        }});\"\"\"),\n",
    "        (\"user\", \"Generate Playwright code for this test plan:\\n{test_plan}\")\n",
    "    ])\n",
    "    chain = prompt | get_llm(0.3) | StrOutputParser()\n",
    "    code = chain.invoke({\"test_plan\": test_plan}).strip()\n",
    "    \n",
    "    # Clean output\n",
    "    code = re.sub(r'^.*?(?=const \\{ test, expect \\} = require)', '', code, flags=re.DOTALL)\n",
    "    code = re.sub(r'^```(?:javascript)?', '', code, flags=re.MULTILINE)\n",
    "    code = re.sub(r'^```$', '', code, flags=re.MULTILINE)\n",
    "    code = code.strip()\n",
    "    \n",
    "    import_statement = \"const { test, expect } = require('@playwright/test');\"\n",
    "    if not code.startswith(\"const { test, expect }\"):\n",
    "        code = f\"{import_statement}\\n{code}\"\n",
    "    \n",
    "    # Remove unwanted waits\n",
    "    code = re.sub(r'\\s*await page\\.waitForLoadState\\(.*?\\);', '', code)\n",
    "    code = re.sub(r'\\s*await page\\.locator\\(.*?\\)\\.waitFor\\(.*?\\);', '', code)\n",
    "    code = re.sub(r'\\s*await page\\.waitForTimeout\\(.*?\\);', '', code)\n",
    "    \n",
    "    lines = code.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if any(line.strip().startswith(prefix) for prefix in [\n",
    "            \"const { test, expect }\", \"test(\", \"async ({ page })\", \"})\", \"await page.\", \"await expect(page.\", \"});\"\n",
    "        ]):\n",
    "            cleaned_lines.append(line)\n",
    "        elif line.strip().startswith(\"//\"):\n",
    "            continue\n",
    "        elif line.strip():\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    code = '\\n'.join(cleaned_lines)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    return {\n",
    "        \"playwright_code\": code,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8124e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_retry\n",
    "def code_reviewer(state: AgentState) -> dict:\n",
    "    \"\"\"Review the generated Playwright code for improvements.\"\"\"\n",
    "    logger.info(\"Agent: CodeReviewer - Reviewing generated code quality.\")\n",
    "    playwright_code = state.get(\"playwright_code\", \"\")\n",
    "    if not playwright_code or playwright_code.startswith(\"// Error:\"):\n",
    "        return {\"review_feedback\": \"No feedback required.\"}\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Review this Playwright test code for quality and accuracy compared to the original code.\n",
    "        Check for:\n",
    "        1. Syntax correctness\n",
    "        2. Playwright best practices\n",
    "        3. Locator reliability\n",
    "        4. Assertion completeness\n",
    "        5. Whether the code accurately reflects the original test structure\n",
    "        6. Whether all original test data values are preserved exactly\n",
    "        7. Whether any unnecessary elements (describe blocks, beforeEach, etc.) have been added\n",
    "        8. Whether any extra comments or explanatory text has been added\n",
    "        If the code is good, executable, and accurately reflects the original, respond EXACTLY with: \"No feedback required.\"\n",
    "        Otherwise, provide specific improvement suggestions.\"\"\"),\n",
    "        (\"user\", \"Review this Playwright code:\\n{playwright_code}\")\n",
    "    ])\n",
    "    chain = prompt | get_llm(0.2) | StrOutputParser()\n",
    "    feedback = chain.invoke({\"playwright_code\": playwright_code}).strip()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    return {\n",
    "        \"review_feedback\": feedback,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fe0b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@with_retry\n",
    "def code_refiner(state: AgentState) -> dict:\n",
    "    \"\"\"Refine the Playwright code based on review feedback.\"\"\"\n",
    "    logger.info(\"ðŸ”§ Agent: CodeRefiner - Applying improvements to code.\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Refine the Playwright code based on the provided feedback.\n",
    "        Requirements:\n",
    "        - Apply all suggested improvements\n",
    "        - Maintain original functionality\n",
    "        - Preserve all original test data values exactly\n",
    "        - Do not add any elements not present in the original code\n",
    "        - Do not add any comments that weren't in the original code\n",
    "        - Do not add any explanatory text before or after the code\n",
    "        - DO NOT add any waitFor, waitForLoadState, or other wait methods that weren't in the original code\n",
    "        - Return ONLY the raw JavaScript code without any formatting or explanations\n",
    "        - The output should be exactly the code that can be saved as a .spec.js file and executed\n",
    "        Example:\n",
    "        const {{ test, expect }} = require('@playwright/test');\n",
    "        test('example', async ({{ page }}) => {{\n",
    "          await page.goto('https://example.com');\n",
    "        }});\"\"\"),\n",
    "        (\"user\", \"Improve this code:\\nOriginal Code:\\n{playwright_code}\\nFeedback:\\n{review_feedback}\")\n",
    "    ])\n",
    "    chain = prompt | get_llm(0.3) | StrOutputParser()\n",
    "    refined_code = chain.invoke({\n",
    "        \"playwright_code\": state[\"playwright_code\"],\n",
    "        \"review_feedback\": state[\"review_feedback\"]\n",
    "    }).strip()\n",
    "    \n",
    "    # Clean refined code\n",
    "    refined_code = re.sub(r'^.*?(?=const \\{ test, expect \\} = require)', '', refined_code, flags=re.DOTALL)\n",
    "    refined_code = re.sub(r'^```(?:\\w+)?', '', refined_code, flags=re.MULTILINE)\n",
    "    refined_code = re.sub(r'^```$', '', refined_code, flags=re.MULTILINE)\n",
    "    \n",
    "    import_statement = \"const { test, expect } = require('@playwright/test');\"\n",
    "    if not refined_code.startswith(\"const { test, expect }\"):\n",
    "        refined_code = f\"{import_statement}\\n{refined_code}\"\n",
    "    \n",
    "    # Remove waits\n",
    "    refined_code = re.sub(r'\\s*await page\\.waitForLoadState\\(.*?\\);', '', refined_code)\n",
    "    refined_code = re.sub(r'\\s*await page\\.locator\\(.*?\\)\\.waitFor\\(.*?\\);', '', refined_code)\n",
    "    refined_code = re.sub(r'\\s*await page\\.waitForTimeout\\(.*?\\);', '', refined_code)\n",
    "    \n",
    "    lines = refined_code.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if any(line.strip().startswith(prefix) for prefix in [\n",
    "            \"const { test, expect }\", \"test(\", \"async ({ page })\", \"})\", \"await page.\", \"await expect(page.\", \"});\"\n",
    "        ]):\n",
    "            cleaned_lines.append(line)\n",
    "        elif line.strip().startswith(\"//\"):\n",
    "            continue\n",
    "        elif line.strip():\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    refined_code = '\\n'.join(cleaned_lines)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    return {\n",
    "        \"final_playwright_code\": refined_code,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed5eac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_final_code_if_needed(state: AgentState) -> dict:\n",
    "    \"\"\"Ensures final_playwright_code is set when no refinement is needed.\"\"\"\n",
    "    feedback = state.get(\"review_feedback\", \"\").lower().strip()\n",
    "    if \"no feedback required\" in feedback:\n",
    "        return {\n",
    "            \"final_playwright_code\": state.get(\"playwright_code\", \"\"),\n",
    "        }\n",
    "    return {}\n",
    "\n",
    "def writer_agent(state: AgentState) -> dict:\n",
    "    \"\"\"Organizes artifacts into project structure folders.\"\"\"\n",
    "    logger.info(\"Agent: WriterAgent - Organizing test files and artifacts.\")\n",
    "    \n",
    "    # Create organized directory structure\n",
    "    output_dir = state[\"output_dir\"]\n",
    "    features_dir = os.path.join(output_dir, \"features\")\n",
    "    tests_dir = os.path.join(output_dir, \"tests\")\n",
    "    reports_dir = os.path.join(output_dir, \"reports\")\n",
    "    \n",
    "    os.makedirs(features_dir, exist_ok=True)\n",
    "    os.makedirs(tests_dir, exist_ok=True)\n",
    "    os.makedirs(reports_dir, exist_ok=True)\n",
    "    \n",
    "    artifacts = {}\n",
    "    \n",
    "    # Save Gherkin feature file\n",
    "    gherkin_feature = state.get(\"gherkin_feature\", \"\")\n",
    "    if gherkin_feature and gherkin_feature.strip():\n",
    "        feature_path = save_artifact(features_dir, \"generated.feature\", gherkin_feature)\n",
    "        if feature_path:\n",
    "            artifacts[\"gherkin_feature\"] = feature_path\n",
    "    \n",
    "    # Save Playwright test code\n",
    "    final_code = state.get(\"final_playwright_code\", \"\")\n",
    "    if final_code and final_code.strip():\n",
    "        test_path = save_artifact(tests_dir, \"generated.spec.js\", final_code)\n",
    "        if test_path:\n",
    "            artifacts[\"playwright_code\"] = test_path\n",
    "    \n",
    "    # Save test execution results if available\n",
    "    test_results = state.get(\"test_results\")\n",
    "    if test_results and test_results.strip():\n",
    "        results_path = save_artifact(reports_dir, \"test_results.txt\", test_results)\n",
    "        if results_path:\n",
    "            artifacts[\"test_results\"] = results_path\n",
    "    \n",
    "    # Save JSON test results if available\n",
    "    test_passed = state.get(\"test_passed\")\n",
    "    if test_passed is not None:\n",
    "        json_results = {\n",
    "            \"test_passed\": test_passed,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        json_path = save_json_artifact(reports_dir, \"test_results.json\", json_results)\n",
    "        if json_path:\n",
    "            artifacts[\"test_results_json\"] = json_path\n",
    "    \n",
    "    # Save coverage report if available\n",
    "    coverage_report = state.get(\"coverage_report\")\n",
    "    if coverage_report and coverage_report.strip():\n",
    "        artifacts[\"coverage_report\"] = coverage_report\n",
    "    \n",
    "    logger.info(f\" Organized {len(artifacts)} artifacts into structured folders.\")\n",
    "    return {\"artifacts\": artifacts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8b8ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def playwright_test_runner(state: AgentState) -> dict:\n",
    "    \"\"\"Execute the final Playwright test script and report results with coverage.\"\"\"\n",
    "    logger.info(\"Agent: PlaywrightTestRunner - Executing test script with coverage.\")\n",
    "    final_code = state.get(\"final_playwright_code\", \"\").strip()\n",
    "    if not final_code:\n",
    "        logger.error(\"No final code to execute\")\n",
    "        return {\"test_results\": \"Error: No valid code to execute.\", \"test_passed\": False}\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
    "    temp_test_file = f\"temp_test_{timestamp}.spec.js\"\n",
    "    temp_test_path = os.path.join(state[\"output_dir\"], \"tests\", temp_test_file)\n",
    "    \n",
    "    try:\n",
    "        with open(temp_test_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(final_code)\n",
    "        logger.info(f\"Test file created: {temp_test_path}\")\n",
    "        \n",
    "        npx_path = shutil.which(\"npx\")\n",
    "        if not npx_path:\n",
    "            potential_paths = [\n",
    "                r\"C:\\Program Files\\nodejs\\npx.cmd\",\n",
    "                r\"C:\\Program Files\\nodejs\\npx\",\n",
    "                os.path.join(os.environ.get(\"APPDATA\", \"\"), \"npm\", \"npx.cmd\"),\n",
    "                os.path.join(os.environ.get(\"PROGRAMFILES\", \"\"), \"nodejs\", \"npx.cmd\"),\n",
    "            ]\n",
    "            for path in potential_paths:\n",
    "                if os.path.exists(path):\n",
    "                    npx_path = path\n",
    "                    break\n",
    "        \n",
    "        if not npx_path:\n",
    "            logger.error(\"Could not find npx executable\")\n",
    "            return {\n",
    "                \"test_results\": \"Error: Could not find npx executable. Please ensure Node.js is properly installed.\",\n",
    "                \"test_passed\": False\n",
    "            }\n",
    "        \n",
    "        # Check if nyc is available\n",
    "        nyc_path = shutil.which(\"nyc\")\n",
    "        use_nyc = nyc_path is not None\n",
    "        \n",
    "        coverage_dir = os.path.join(state[\"output_dir\"], \"reports\", \"coverage\")\n",
    "        os.makedirs(coverage_dir, exist_ok=True)\n",
    "        coverage_percentage = 0.0\n",
    "        coverage_report_path = \"\"\n",
    "        \n",
    "        env = os.environ.copy()\n",
    "        path_separator = \";\" if os.name == \"nt\" else \":\"\n",
    "        env[\"PATH\"] = f\"{os.path.dirname(npx_path)}{path_separator}{env.get('PATH', '')}\"\n",
    "        env[\"DEBUG\"] = \"pw:api\"\n",
    "        \n",
    "        if use_nyc:\n",
    "            logger.info(\"Using nyc for coverage reporting...\")\n",
    "            # Create a temporary package.json for nyc\n",
    "            package_json_path = os.path.join(state[\"output_dir\"], \"package.json\")\n",
    "            package_json = {\n",
    "                \"name\": \"test-coverage\",\n",
    "                \"version\": \"1.0.0\",\n",
    "                \"scripts\": {\n",
    "                    \"test\": \"nyc --reporter=text --reporter=html --report-dir=./reports/coverage npx playwright test \" + temp_test_path\n",
    "                },\n",
    "                \"devDependencies\": {\n",
    "                    \"@playwright/test\": \"^1.30.0\",\n",
    "                    \"nyc\": \"^15.1.0\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            with open(package_json_path, \"w\") as f:\n",
    "                json.dump(package_json, f, indent=2)\n",
    "            \n",
    "            # Create a .nycrc configuration file\n",
    "            nycrc_path = os.path.join(state[\"output_dir\"], \".nycrc\")\n",
    "            nycrc_config = {\n",
    "                \"reporter\": [\"text\", \"html\"],\n",
    "                \"report-dir\": \"./reports/coverage\",\n",
    "                \"include\": [\"**/*.js\"],\n",
    "                \"exclude\": [\"**/node_modules/**\", \"**/tests/**\", \"**/coverage/**\"],\n",
    "                \"all\": True,\n",
    "                \"check-coverage\": False,\n",
    "                \"lines\": 80,\n",
    "                \"statements\": 80,\n",
    "                \"functions\": 80,\n",
    "                \"branches\": 80\n",
    "            }\n",
    "            \n",
    "            with open(nycrc_path, \"w\") as f:\n",
    "                json.dump(nycrc_config, f, indent=2)\n",
    "            \n",
    "            # Run nyc to collect coverage\n",
    "            nyc_command = [nyc_path, \"--reporter=text\", \"--reporter=html\", \"--report-dir=./reports/coverage\", \"npx\", \"playwright\", \"test\", temp_test_path]\n",
    "            result = subprocess.run(nyc_command, capture_output=True, text=True, timeout=120, env=env, cwd=state[\"output_dir\"])\n",
    "            \n",
    "            # Extract coverage percentage from nyc output\n",
    "            coverage_match = re.search(r'All files\\s*\\|\\s*(\\d+\\.\\d+)', result.stdout)\n",
    "            if coverage_match:\n",
    "                coverage_percentage = float(coverage_match.group(1))\n",
    "                logger.info(f\"Coverage: {coverage_percentage:.1f}%\")\n",
    "            \n",
    "            # Save coverage report path\n",
    "            coverage_report_path = os.path.join(coverage_dir, \"index.html\")\n",
    "            if os.path.exists(coverage_report_path):\n",
    "                logger.info(f\"Coverage report saved to: {coverage_report_path}\")\n",
    "            \n",
    "            test_passed = result.returncode == 0\n",
    "            status = \"PASSED\" if test_passed else \"FAILED\"\n",
    "            logger.info(f\"Test execution completed: {status}\")\n",
    "            \n",
    "            # The output from nyc includes both test output and coverage\n",
    "            execution_summary = f\"\"\"\n",
    "Test Execution Summary (with coverage):\n",
    "  Status: {status}\n",
    "  Test File: {temp_test_path}\n",
    "  Return Code: {result.returncode}\n",
    "  Coverage: {coverage_percentage:.1f}%\n",
    "  \n",
    "Output:\n",
    "{result.stdout}\n",
    "\n",
    "Errors:\n",
    "{result.stderr}\n",
    "\"\"\"\n",
    "        else:\n",
    "            logger.info(\" Running tests without coverage (nyc not available)...\")\n",
    "            # Run tests without coverage\n",
    "            command = [npx_path, \"playwright\", \"test\", temp_test_path, \"--reporter=list\", \"--timeout=30000\", \"--retries=0\"]\n",
    "            result = subprocess.run(command, capture_output=True, text=True, timeout=120, env=env)\n",
    "            \n",
    "            test_passed = result.returncode == 0\n",
    "            status = \"PASSED\" if test_passed else \"FAILED\"\n",
    "            logger.info(f\"Test execution completed: {status}\")\n",
    "            \n",
    "            # Create a simple coverage summary\n",
    "            coverage_summary = f\"\"\"\n",
    "Coverage Summary:\n",
    "  Test Status: {status}\n",
    "  Test File: {temp_test_path}\n",
    "  Coverage Report: Not available (nyc/Istanbul not installed)\n",
    "  \n",
    "Test Output:\n",
    "{result.stdout}\n",
    "\n",
    "Test Errors:\n",
    "{result.stderr}\n",
    "\"\"\"\n",
    "            coverage_report_path = save_artifact(coverage_dir, \"coverage_summary.txt\", coverage_summary)\n",
    "            \n",
    "            execution_summary = f\"\"\"\n",
    "Test Execution Summary:\n",
    "  Status: {status}\n",
    "  Test File: {temp_test_path}\n",
    "  Return Code: {result.returncode}\n",
    "  \n",
    "Output:\n",
    "{result.stdout}\n",
    "\n",
    "Errors:\n",
    "{result.stderr}\n",
    "\"\"\"\n",
    "        \n",
    "        return {\n",
    "            \"test_results\": execution_summary,\n",
    "            \"test_passed\": test_passed,\n",
    "            \"coverage_report\": coverage_report_path,\n",
    "            \"coverage_percentage\": coverage_percentage,\n",
    "        }\n",
    "    except subprocess.TimeoutExpired:\n",
    "        error_msg = \"Test execution timed out after 2 minutes\"\n",
    "        logger.error(error_msg)\n",
    "        return {\n",
    "            \"test_results\": f\"Error: {error_msg}\",\n",
    "            \"test_passed\": False,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Test execution failed: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return {\n",
    "            \"test_results\": f\"Error: {error_msg}\",\n",
    "            \"test_passed\": False,\n",
    "        }\n",
    "    finally:\n",
    "        if os.path.exists(temp_test_path):\n",
    "            try:\n",
    "                os.remove(temp_test_path)\n",
    "                logger.info(f\"Cleaned up: {temp_test_path}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Cleanup failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6d2d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Build Graph\n",
    "def decide_to_refine(state: AgentState) -> str:\n",
    "    feedback = state.get(\"review_feedback\", \"\").lower().strip()\n",
    "    if \"no feedback required\" in feedback:\n",
    "        logger.info(\"No refinement needed\")\n",
    "        return \"set_final_code\"\n",
    "    else:\n",
    "        logger.info(\"Code needs refinement\")\n",
    "        return \"refine_code\"\n",
    "\n",
    "def build_graph():\n",
    "    workflow = StateGraph(AgentState)\n",
    "    workflow.add_node(\"synthetic_user_story_generator\", synthetic_user_story_generator)\n",
    "    workflow.add_node(\"gherkin_converter\", gherkin_converter)\n",
    "    workflow.add_node(\"code_analyzer\", code_analyzer)\n",
    "    workflow.add_node(\"test_plan_generator\", test_plan_generator)\n",
    "    workflow.add_node(\"playwright_code_generator\", playwright_code_generator)\n",
    "    workflow.add_node(\"code_reviewer\", code_reviewer)\n",
    "    workflow.add_node(\"code_refiner\", code_refiner)\n",
    "    workflow.add_node(\"set_final_code\", set_final_code_if_needed)\n",
    "    workflow.add_node(\"writer_agent\", writer_agent)\n",
    "    workflow.add_node(\"playwright_test_runner\", playwright_test_runner)\n",
    "    \n",
    "    workflow.set_entry_point(\"synthetic_user_story_generator\")\n",
    "    workflow.add_edge(\"synthetic_user_story_generator\", \"gherkin_converter\")\n",
    "    workflow.add_edge(\"gherkin_converter\", \"code_analyzer\")\n",
    "    workflow.add_edge(\"code_analyzer\", \"test_plan_generator\")\n",
    "    workflow.add_edge(\"test_plan_generator\", \"playwright_code_generator\")\n",
    "    workflow.add_edge(\"playwright_code_generator\", \"code_reviewer\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"code_reviewer\",\n",
    "        decide_to_refine,\n",
    "        {\n",
    "            \"refine_code\": \"code_refiner\",\n",
    "            \"set_final_code\": \"set_final_code\"\n",
    "        }\n",
    "    )\n",
    "    workflow.add_edge(\"code_refiner\", \"writer_agent\")\n",
    "    workflow.add_edge(\"set_final_code\", \"writer_agent\")\n",
    "    workflow.add_edge(\"writer_agent\", \"playwright_test_runner\")\n",
    "    workflow.add_edge(\"playwright_test_runner\", END)\n",
    "    return workflow.compile()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da8315a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Test Case Processing\n",
    "def process_test_case(test_case: dict):\n",
    "    try:\n",
    "        case_name = test_case['name']\n",
    "        logger.info(f\"Processing: {case_name}\")\n",
    "        start_time = datetime.now()\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Create output directory for this test case\n",
    "        output_dir = os.path.join(OUTPUT_FOLDER, f\"test_case_{case_name}_{timestamp}\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories\n",
    "        for sub in [\"features\", \"tests\", \"reports\"]:\n",
    "            os.makedirs(os.path.join(output_dir, sub), exist_ok=True)\n",
    "        \n",
    "        app = build_graph()\n",
    "        initial_state = AgentState(\n",
    "            user_story=test_case[\"user_story\"],\n",
    "            code_input=test_case[\"code_input\"],\n",
    "            code_path=test_case[\"code_path\"],\n",
    "            gherkin_feature=\"\",\n",
    "            analyzed_code=\"\",\n",
    "            test_plan=\"\",\n",
    "            playwright_code=\"\",\n",
    "            review_feedback=\"\",\n",
    "            final_playwright_code=\"\",\n",
    "            test_results=None,\n",
    "            test_passed=None,\n",
    "            coverage_report=None,\n",
    "            coverage_percentage=None,\n",
    "            output_dir=output_dir,\n",
    "            artifacts={}\n",
    "        )\n",
    "        \n",
    "        result = app.invoke(initial_state)\n",
    "        \n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        final_code = result.get(\"final_playwright_code\", \"\")\n",
    "        if final_code:\n",
    "            logger.info(f\"{case_name}: Generated executable code ({len(final_code)} chars)\")\n",
    "        else:\n",
    "            logger.warning(f\" {case_name}: No final code generated\")\n",
    "        \n",
    "        if result.get(\"test_passed\") is not None:\n",
    "            status = \"PASSED\" if result[\"test_passed\"] else \"FAILED\"\n",
    "            logger.info(f\"{case_name}: Test {status}\")\n",
    "        \n",
    "        coverage = result.get(\"coverage_percentage\")\n",
    "        if coverage is not None:\n",
    "            logger.info(f\"{case_name}: Coverage {coverage:.1f}%\")\n",
    "        \n",
    "        logger.info(f\" {case_name}: Completed in {duration:.2f}s\")\n",
    "        return case_name, result, output_dir\n",
    "    except Exception as e:\n",
    "        logger.error(f\" FATAL ERROR in {test_case['name']}: {e}\", exc_info=True)\n",
    "        return test_case[\"name\"], None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "281bfbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 08:33:37,353 [INFO] - Starting Universal Test Converter...\n",
      "2025-08-05 08:33:37,355 [INFO] - Loading test cases from: C:\\Projects\\LG\\input_data_1\n",
      "2025-08-05 08:33:37,377 [INFO] - Loaded: components\n",
      "2025-08-05 08:33:37,379 [WARNING] - Incomplete: pages\n",
      "2025-08-05 08:33:37,380 [WARNING] - Incomplete: styles\n",
      "2025-08-05 08:33:37,403 [INFO] - Loaded: test_case_1\n",
      "2025-08-05 08:33:37,424 [INFO] - Loaded: test_case_2\n",
      "2025-08-05 08:33:37,427 [INFO] -  Total loaded: 3 test cases\n",
      "2025-08-05 08:33:37,428 [INFO] - Found 3 test cases to process\n",
      "2025-08-05 08:33:37,429 [INFO] - \n",
      "Processing 1/3: components\n",
      "2025-08-05 08:33:37,430 [INFO] - Processing: components\n",
      "2025-08-05 08:33:37,458 [INFO] -  Agent: SyntheticUserStoryGenerator - Creating user story from code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_==========================================================_\n",
      "UNIVERSAL TEST CONVERTER \n",
      "****==========================================================****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 08:33:38,995 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:33:40,006 [INFO] - Agent: GherkinConverter - Converting user story to Gherkin BDD format.\n",
      "2025-08-05 08:33:41,221 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:33:41,226 [INFO] - Agent: CodeAnalyzer - Analyzing input code.\n",
      "2025-08-05 08:33:43,605 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:33:45,612 [INFO] - Agent: TestPlanGenerator - Creating comprehensive test plan.\n",
      "2025-08-05 08:33:47,860 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:33:49,864 [INFO] - Agent: PlaywrightCodeGenerator - Generating executable test code.\n",
      "2025-08-05 08:33:51,322 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:33:53,329 [INFO] - Agent: CodeReviewer - Reviewing generated code quality.\n",
      "2025-08-05 08:33:55,300 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:33:57,303 [INFO] - No refinement needed\n",
      "2025-08-05 08:33:57,309 [INFO] - Agent: WriterAgent - Organizing test files and artifacts.\n",
      "2025-08-05 08:33:57,314 [INFO] - Saved artifact: input_data_output_16\\test_case_components_20250805_083337\\features\\generated.feature\n",
      "2025-08-05 08:33:57,316 [INFO] - Saved artifact: input_data_output_16\\test_case_components_20250805_083337\\tests\\generated.spec.js\n",
      "2025-08-05 08:33:57,317 [INFO] -  Organized 2 artifacts into structured folders.\n",
      "2025-08-05 08:33:57,318 [INFO] - Agent: PlaywrightTestRunner - Executing test script with coverage.\n",
      "2025-08-05 08:33:57,320 [INFO] - Test file created: input_data_output_16\\test_case_components_20250805_083337\\tests\\temp_test_20250805_083357_319077.spec.js\n",
      "2025-08-05 08:33:57,332 [INFO] -  Running tests without coverage (nyc not available)...\n",
      "2025-08-05 08:33:58,925 [INFO] - Test execution completed: FAILED\n",
      "2025-08-05 08:33:58,926 [INFO] - Saved artifact: input_data_output_16\\test_case_components_20250805_083337\\reports\\coverage\\coverage_summary.txt\n",
      "2025-08-05 08:33:58,927 [INFO] - Cleaned up: input_data_output_16\\test_case_components_20250805_083337\\tests\\temp_test_20250805_083357_319077.spec.js\n",
      "2025-08-05 08:33:58,930 [INFO] - components: Generated executable code (435 chars)\n",
      "2025-08-05 08:33:58,931 [INFO] - components: Test FAILED\n",
      "2025-08-05 08:33:58,932 [INFO] - components: Coverage 0.0%\n",
      "2025-08-05 08:33:58,932 [INFO] -  components: Completed in 21.50s\n",
      "2025-08-05 08:33:58,933 [INFO] - Waiting 5 seconds to respect rate limits...\n",
      "2025-08-05 08:34:03,934 [INFO] - \n",
      "Processing 2/3: test_case_1\n",
      "2025-08-05 08:34:03,936 [INFO] - Processing: test_case_1\n",
      "2025-08-05 08:34:03,961 [INFO] -  Agent: SyntheticUserStoryGenerator - Creating user story from code.\n",
      "2025-08-05 08:34:05,047 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:34:06,051 [INFO] - Agent: GherkinConverter - Converting user story to Gherkin BDD format.\n",
      "2025-08-05 08:34:07,461 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:34:07,466 [INFO] - Agent: CodeAnalyzer - Analyzing input code.\n",
      "2025-08-05 08:34:09,670 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:34:11,677 [INFO] - Agent: TestPlanGenerator - Creating comprehensive test plan.\n",
      "2025-08-05 08:34:13,924 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:34:15,928 [INFO] - Agent: PlaywrightCodeGenerator - Generating executable test code.\n",
      "2025-08-05 08:34:17,406 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:34:19,412 [INFO] - Agent: CodeReviewer - Reviewing generated code quality.\n",
      "2025-08-05 08:34:21,981 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:34:23,985 [INFO] - No refinement needed\n",
      "2025-08-05 08:34:23,990 [INFO] - Agent: WriterAgent - Organizing test files and artifacts.\n",
      "2025-08-05 08:34:23,994 [INFO] - Saved artifact: input_data_output_16\\test_case_test_case_1_20250805_083403\\features\\generated.feature\n",
      "2025-08-05 08:34:23,996 [INFO] - Saved artifact: input_data_output_16\\test_case_test_case_1_20250805_083403\\tests\\generated.spec.js\n",
      "2025-08-05 08:34:23,997 [INFO] -  Organized 2 artifacts into structured folders.\n",
      "2025-08-05 08:34:23,998 [INFO] - Agent: PlaywrightTestRunner - Executing test script with coverage.\n",
      "2025-08-05 08:34:24,000 [INFO] - Test file created: input_data_output_16\\test_case_test_case_1_20250805_083403\\tests\\temp_test_20250805_083423_999567.spec.js\n",
      "2025-08-05 08:34:24,010 [INFO] -  Running tests without coverage (nyc not available)...\n",
      "2025-08-05 08:34:25,530 [INFO] - Test execution completed: FAILED\n",
      "2025-08-05 08:34:25,533 [INFO] - Saved artifact: input_data_output_16\\test_case_test_case_1_20250805_083403\\reports\\coverage\\coverage_summary.txt\n",
      "2025-08-05 08:34:25,536 [INFO] - Cleaned up: input_data_output_16\\test_case_test_case_1_20250805_083403\\tests\\temp_test_20250805_083423_999567.spec.js\n",
      "2025-08-05 08:34:25,538 [INFO] - test_case_1: Generated executable code (658 chars)\n",
      "2025-08-05 08:34:25,540 [INFO] - test_case_1: Test FAILED\n",
      "2025-08-05 08:34:25,541 [INFO] - test_case_1: Coverage 0.0%\n",
      "2025-08-05 08:34:25,542 [INFO] -  test_case_1: Completed in 21.60s\n",
      "2025-08-05 08:34:25,545 [INFO] - Waiting 5 seconds to respect rate limits...\n",
      "2025-08-05 08:34:30,546 [INFO] - \n",
      "Processing 3/3: test_case_2\n",
      "2025-08-05 08:34:30,548 [INFO] - Processing: test_case_2\n",
      "2025-08-05 08:34:30,580 [INFO] -  Agent: SyntheticUserStoryGenerator - Creating user story from code.\n",
      "2025-08-05 08:34:31,696 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:34:32,698 [INFO] - Agent: GherkinConverter - Converting user story to Gherkin BDD format.\n",
      "2025-08-05 08:34:34,181 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:34:34,184 [INFO] - Agent: CodeAnalyzer - Analyzing input code.\n",
      "2025-08-05 08:34:36,393 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:34:38,396 [INFO] - Agent: TestPlanGenerator - Creating comprehensive test plan.\n",
      "2025-08-05 08:34:40,437 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:34:42,442 [INFO] - Agent: PlaywrightCodeGenerator - Generating executable test code.\n",
      "2025-08-05 08:34:43,163 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-05 08:34:43,167 [INFO] - Retrying request to /openai/v1/chat/completions in 4.000000 seconds\n",
      "2025-08-05 08:34:48,440 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:34:50,445 [INFO] - Agent: CodeReviewer - Reviewing generated code quality.\n",
      "2025-08-05 08:34:51,065 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-05 08:34:51,066 [INFO] - Retrying request to /openai/v1/chat/completions in 5.000000 seconds\n",
      "2025-08-05 08:34:57,552 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:34:59,557 [INFO] - Code needs refinement\n",
      "2025-08-05 08:34:59,560 [INFO] - ðŸ”§ Agent: CodeRefiner - Applying improvements to code.\n",
      "2025-08-05 08:35:00,212 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-05 08:35:00,214 [INFO] - Retrying request to /openai/v1/chat/completions in 8.000000 seconds\n",
      "2025-08-05 08:35:09,356 [INFO] - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-05 08:35:11,360 [INFO] - Agent: WriterAgent - Organizing test files and artifacts.\n",
      "2025-08-05 08:35:11,368 [INFO] - Saved artifact: input_data_output_16\\test_case_test_case_2_20250805_083430\\features\\generated.feature\n",
      "2025-08-05 08:35:11,371 [INFO] - Saved artifact: input_data_output_16\\test_case_test_case_2_20250805_083430\\tests\\generated.spec.js\n",
      "2025-08-05 08:35:11,373 [INFO] -  Organized 2 artifacts into structured folders.\n",
      "2025-08-05 08:35:11,375 [INFO] - Agent: PlaywrightTestRunner - Executing test script with coverage.\n",
      "2025-08-05 08:35:11,377 [INFO] - Test file created: input_data_output_16\\test_case_test_case_2_20250805_083430\\tests\\temp_test_20250805_083511_376556.spec.js\n",
      "2025-08-05 08:35:11,389 [INFO] -  Running tests without coverage (nyc not available)...\n",
      "2025-08-05 08:35:12,817 [INFO] - Test execution completed: FAILED\n",
      "2025-08-05 08:35:12,821 [INFO] - Saved artifact: input_data_output_16\\test_case_test_case_2_20250805_083430\\reports\\coverage\\coverage_summary.txt\n",
      "2025-08-05 08:35:12,824 [INFO] - Cleaned up: input_data_output_16\\test_case_test_case_2_20250805_083430\\tests\\temp_test_20250805_083511_376556.spec.js\n",
      "2025-08-05 08:35:12,826 [INFO] - test_case_2: Generated executable code (897 chars)\n",
      "2025-08-05 08:35:12,828 [INFO] - test_case_2: Test FAILED\n",
      "2025-08-05 08:35:12,830 [INFO] - test_case_2: Coverage 0.0%\n",
      "2025-08-05 08:35:12,832 [INFO] -  test_case_2: Completed in 42.28s\n",
      "2025-08-05 08:35:12,837 [INFO] - Saved JSON artifact: input_data_output_16\\final_report.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================_\n",
      "FINAL REPORT \n",
      "_==========================================================_\n",
      "  components: FAILED (Coverage: 0.0%)\n",
      "  test_case_1: FAILED (Coverage: 0.0%)\n",
      "  test_case_2: FAILED (Coverage: 0.0%)\n",
      "\n",
      " STATISTICS:\n",
      "  â€¢ Total Cases: 3\n",
      "  â€¢ Tests Passed: 0\n",
      "  â€¢ Tests Failed: 3\n",
      "  â€¢ Incomplete: 0\n",
      "  â€¢ Success Rate: 0.0%\n",
      "  â€¢ Average Coverage: 0.0%\n",
      "  â€¢ Total Time: 95.48s\n",
      "  â€¢ Avg Time/Case: 31.83s\n",
      "\n",
      " Universal Test Converter completed! \n",
      "Check the 'input_data_output_16' folder for all generated artifacts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = datetime.now()\n",
    "    print(\"_\" + \"=\" * 58 + \"_\")\n",
    "    print(\"UNIVERSAL TEST CONVERTER \")\n",
    "    print(\"****\" + \"=\" * 58 + \"****\")\n",
    "    logger.info(\"Starting Universal Test Converter...\")\n",
    "    \n",
    "    test_cases = load_test_cases(\"C:\\\\Projects\\\\LG\\\\input_data_1\")  # Update path if needed input_data\n",
    "    if not test_cases:\n",
    "        logger.error(\" No test cases found!\")\n",
    "    else:\n",
    "        logger.info(f\"Found {len(test_cases)} test cases to process\")\n",
    "        results = []\n",
    "        output_dirs = []\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases, 1):\n",
    "            logger.info(f\"\\nProcessing {i}/{len(test_cases)}: {test_case['name']}\")\n",
    "            name, result, output_dir = process_test_case(test_case)\n",
    "            results.append((name, result))\n",
    "            output_dirs.append(output_dir)\n",
    "            \n",
    "            if i < len(test_cases):\n",
    "                logger.info(\"Waiting 5 seconds to respect rate limits...\")\n",
    "                time.sleep(5)\n",
    "        \n",
    "        # Final Report\n",
    "        total_time = (datetime.now() - start_time).total_seconds()\n",
    "        print(\"\\n\" + \"\" + \"=\" * 58 + \"_\")\n",
    "        print(\"FINAL REPORT \")\n",
    "        print(\"_\" + \"=\" * 58 + \"_\")\n",
    "        \n",
    "        passed_count = sum(1 for _, r in results if r and r.get(\"test_passed\") is True)\n",
    "        failed_count = sum(1 for _, r in results if r and r.get(\"test_passed\") is False)\n",
    "        incomplete_count = len(results) - passed_count - failed_count\n",
    "        \n",
    "        # Calculate average coverage\n",
    "        coverage_values = [r.get(\"coverage_percentage\") for _, r in results if r and r.get(\"coverage_percentage\") is not None]\n",
    "        avg_coverage = sum(coverage_values) / len(coverage_values) if coverage_values else 0.0\n",
    "        \n",
    "        for name, result in results:\n",
    "            status = \"PASSED\" if result and result.get(\"test_passed\") else \"FAILED\"\n",
    "            if not result or not result.get(\"final_playwright_code\"):\n",
    "                status = \"INCOMPLETE\"\n",
    "            \n",
    "            coverage = result.get(\"coverage_percentage\")\n",
    "            coverage_str = f\" (Coverage: {coverage:.1f}%)\" if coverage is not None else \"\"\n",
    "            print(f\"  {name}: {status}{coverage_str}\")\n",
    "        \n",
    "        print(f\"\\n STATISTICS:\")\n",
    "        print(f\"  â€¢ Total Cases: {len(results)}\")\n",
    "        print(f\"  â€¢ Tests Passed: {passed_count}\")\n",
    "        print(f\"  â€¢ Tests Failed: {failed_count}\")\n",
    "        print(f\"  â€¢ Incomplete: {incomplete_count}\")\n",
    "        print(f\"  â€¢ Success Rate: {(passed_count / len(results) * 100):.1f}%\")\n",
    "        print(f\"  â€¢ Average Coverage: {avg_coverage:.1f}%\")\n",
    "        print(f\"  â€¢ Total Time: {total_time:.2f}s\")\n",
    "        print(f\"  â€¢ Avg Time/Case: {total_time / len(results):.2f}s\")\n",
    "        \n",
    "        # Create final report\n",
    "        report_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"total_time\": total_time,\n",
    "            \"total_cases\": len(results),\n",
    "            \"passed\": passed_count,\n",
    "            \"failed\": failed_count,\n",
    "            \"incomplete\": incomplete_count,\n",
    "            \"success_rate\": (passed_count / len(results) * 100),\n",
    "            \"average_coverage\": avg_coverage,\n",
    "            \"avg_time_per_case\": total_time / len(results),\n",
    "            \"output_dirs\": output_dirs\n",
    "        }\n",
    "        \n",
    "        # Save final report\n",
    "        save_json_artifact(OUTPUT_FOLDER, \"final_report.json\", report_data)\n",
    "    \n",
    "    print(\"\\n Universal Test Converter completed! \")\n",
    "    print(f\"Check the '{OUTPUT_FOLDER}' folder for all generated artifacts.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195e2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ac4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
